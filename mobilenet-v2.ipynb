{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-21T21:05:27.010926Z","iopub.status.busy":"2023-03-21T21:05:27.010319Z","iopub.status.idle":"2023-03-21T21:05:28.099948Z","shell.execute_reply":"2023-03-21T21:05:28.098764Z","shell.execute_reply.started":"2023-03-21T21:05:27.010890Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Mar 21 21:05:27 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    34W / 250W |   1587MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.102831Z","iopub.status.busy":"2023-03-21T21:05:28.102524Z","iopub.status.idle":"2023-03-21T21:05:28.116469Z","shell.execute_reply":"2023-03-21T21:05:28.115149Z","shell.execute_reply.started":"2023-03-21T21:05:28.102798Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch version: 1.13.0\n","torchvision version: 0.14.0\n"]}],"source":["import os\n","from time import time\n","from tqdm import tqdm\n","import numpy\n","import pandas as pd\n","\n","import torch\n","import torch\n","from torch.nn import Linear, CrossEntropyLoss\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","from torchvision.datasets import ImageFolder\n","from torchvision.models import resnet18\n","from torchvision.transforms import transforms\n","\n","# Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","import tifffile\n","try:\n","    import torch\n","    import torchvision\n","    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n","    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")\n","except:\n","    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n","    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n","    import torch\n","    import torchvision\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.118678Z","iopub.status.busy":"2023-03-21T21:05:28.118259Z","iopub.status.idle":"2023-03-21T21:05:28.458544Z","shell.execute_reply":"2023-03-21T21:05:28.457456Z","shell.execute_reply.started":"2023-03-21T21:05:28.118643Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"]},{"name":"stdout","output_type":"stream","text":["True\n","True\n"]}],"source":["import torchvision.models as models\n","import torch\n","model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n","model.eval()\n","\n","# define VGG16 model\n","mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n","\n","# check if CUDA is available\n","use_cuda = torch.cuda.is_available()\n","print(use_cuda)\n","# move model to GPU if CUDA is available\n","if use_cuda:\n","    mobilenet_v2 = mobilenet_v2.cuda()\n","# define VGG16 model\n","mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n","\n","# check if CUDA is available\n","use_cuda = torch.cuda.is_available()\n","print(use_cuda)\n","# move model to GPU if CUDA is available\n","if use_cuda:\n","    mobilenet_v2 = mobilenet_v2.cuda()"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:15:42.727996Z","iopub.status.busy":"2023-03-21T21:15:42.727636Z","iopub.status.idle":"2023-03-21T21:15:42.756703Z","shell.execute_reply":"2023-03-21T21:15:42.755621Z","shell.execute_reply.started":"2023-03-21T21:15:42.727965Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n","800\n","400\n","5\n"]}],"source":["import os\n","from torchvision import datasets\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","import time\n","from PIL import Image\n","import torchvision.transforms as transforms\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","from torch.utils.data import random_split\n","\n","### TODO: Write data loaders for training, validation, and test sets\n","## Specify appropriate transforms, and batch_sizes\n","\n","\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 32\n","\n","\n","data_dir = '/kaggle/input/messidor1-data/P_Data'\n","train_dir = os.path.join(data_dir, 'Train/')\n","test_dir = os.path.join(data_dir, 'Test/')\n","\n","# percentage of training set to use as validation\n","valid_size = 0.2\n","\n","\n","transform = { 'train'}\n","\n","data_transforms = {\n","    'train' : transforms.Compose([transforms.Resize(128),\n","                                  transforms.CenterCrop(128),\n","                                  transforms.RandomHorizontalFlip(), # randomly flip and rotate\n","                                  transforms.RandomRotation(10),\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.3288, 0.1636, 0.0559],\n","                                  std=[0.3691, 0.1875, 0.0723])]),\n","\n","    \n","    'test' : transforms.Compose([transforms.Resize(128),\n","                                  transforms.CenterCrop(128),\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize(mean=[0.3288, 0.1636, 0.0559],\n","                                  std=[0.3691, 0.1875, 0.0723])])}\n","\n","image_datasets = {'train' : datasets.ImageFolder(root=train_dir,transform=data_transforms['train']),\n","                  \n","                  'test' : datasets.ImageFolder(root=test_dir,transform=data_transforms['test']),}\n","\n","# split the training set into training and validation sets\n","train_dataset, valid_dataset = random_split(image_datasets['train'], \n","                                            [int((1 - valid_size) * len(image_datasets['train'])), \n","                                             int(valid_size * len(image_datasets['train']))])\n","# define the data loaders for the training, validation, and test sets\n","loaders_transfer = {\n","    'train': torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True),\n","    'valid': torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True),\n","    'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train',  'test']}\n","class_names = image_datasets['train'].classes\n","n_classes = len(class_names)\n","\n","print( len(class_names))\n","print( len(image_datasets['train']))\n","\n","print( len(image_datasets['test']))\n","print( len(loaders_transfer['valid']))"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.499018Z","iopub.status.busy":"2023-03-21T21:05:28.498758Z","iopub.status.idle":"2023-03-21T21:05:28.504387Z","shell.execute_reply":"2023-03-21T21:05:28.503185Z","shell.execute_reply.started":"2023-03-21T21:05:28.498993Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["print(len(mobilenet_v2.classifier))"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.507075Z","iopub.status.busy":"2023-03-21T21:05:28.506222Z","iopub.status.idle":"2023-03-21T21:05:28.517150Z","shell.execute_reply":"2023-03-21T21:05:28.516166Z","shell.execute_reply.started":"2023-03-21T21:05:28.507040Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["MobileNetV2(\n","  (features): Sequential(\n","    (0): Conv2dNormActivation(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","    (1): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n","          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (7): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n","          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (8): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (9): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (10): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (11): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (12): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (13): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (14): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (15): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (16): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (17): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n","          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU6(inplace=True)\n","        )\n","        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (18): Conv2dNormActivation(\n","      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.2, inplace=False)\n","    (1): Linear(in_features=1280, out_features=1000, bias=True)\n","  )\n",")\n","1280\n","1000\n"]}],"source":["print(mobilenet_v2)\n","print(mobilenet_v2.classifier[1].in_features) \n","print(mobilenet_v2.classifier[1].out_features)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.519387Z","iopub.status.busy":"2023-03-21T21:05:28.518523Z","iopub.status.idle":"2023-03-21T21:05:28.631133Z","shell.execute_reply":"2023-03-21T21:05:28.630177Z","shell.execute_reply.started":"2023-03-21T21:05:28.519351Z"},"trusted":true},"outputs":[],"source":["import torchvision.models as models\n","import torch.nn as nn\n","\n","\n","## TODO: Specify model architecture \n","\n","# Load the pretrained model from pytorch\n","model_transfer = models.mobilenet_v2(pretrained=True)\n","\n","feats_list = list(model_transfer.features)\n","new_feats_list = []\n","for feat in feats_list:\n","    new_feats_list.append(feat)\n","    if isinstance(feat, nn.Conv2d):\n","        new_feats_list.append(nn.Dropout(p=0.5, inplace=True))\n","\n","# modify convolution layers\n","model_transfer.features = nn.Sequential(*new_feats_list)\n","\n","\n","for param in model_transfer.features.parameters():\n","    param.requires_grad = False\n","\n","\n","n_inputs = model_transfer.classifier[1].in_features\n","\n","last_layer = nn.Linear(n_inputs, 4)\n","\n","model_transfer.classifier[1] = last_layer\n","if use_cuda:\n","    model_transfer = model_transfer.cuda()"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.632963Z","iopub.status.busy":"2023-03-21T21:05:28.632512Z","iopub.status.idle":"2023-03-21T21:05:28.639333Z","shell.execute_reply":"2023-03-21T21:05:28.637885Z","shell.execute_reply.started":"2023-03-21T21:05:28.632917Z"},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","\n","criterion_transfer = nn.CrossEntropyLoss()\n","optimizer_transfer = optim.Adam(model_transfer.classifier.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.641415Z","iopub.status.busy":"2023-03-21T21:05:28.640871Z","iopub.status.idle":"2023-03-21T21:05:28.657914Z","shell.execute_reply":"2023-03-21T21:05:28.656728Z","shell.execute_reply.started":"2023-03-21T21:05:28.641372Z"},"trusted":true},"outputs":[],"source":["train_loss_list =[]\n","train_acc_list = []\n","def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n","    \"\"\"returns trained model\"\"\"\n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = np.Inf \n","    total_time = 0\n","    for epoch in range(1, n_epochs+1):\n","        # initialize variables to monitor training and validation loss\n","        train_loss  = 0.0\n","        correct_train = 0.\n","        total_train = 0.\n","        \n","        valid_loss = 0.0\n","\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train()\n","        start = timeit.default_timer()\n","        for batch_idx, (data, target) in enumerate(loaders['train']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            ## find the loss and update the model parameters accordingly\n","            ## record the average training loss, using something like\n","            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","            # clear the gradients of all optimized variables\n","            optimizer.zero_grad()\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(data)\n","            pred = output.data.max(1, keepdim=True)[1] #miraj\n","            # _, predicted = torch.max(output.data, 1)\n","            # calculate the batch loss\n","            loss = criterion(output, target)\n","            # backward pass: compute gradient of the loss with respect to model parameters\n","            loss.backward()\n","            # perform a single optimization step (parameter update)\n","            optimizer.step()\n","            # update training loss\n","            train_loss += loss.item()\n","            total_train += target.size(0)\n","            correct_train += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())#(predicted == target).sum().item()\n","            \n","          \n","            data.size(0)\n","            ###\n","        stop = timeit.default_timer() \n","        new_time = stop-start\n","        total_time+=new_time\n","        ######################\n","        # validate the model #\n","        ######################\n","        model.eval()\n","\n","        for batch_idx, (data, target) in enumerate(loaders['valid']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            # update the average validation loss\n","\n","            ##\n","            #forward pass: compute predicted outputs by passing inputs to the model\n","            output = model(data)\n","            preds = torch.nn.functional.log_softmax(output , dim=1)\n","\n","            #calculate the batch loss\n","            loss = criterion(output, target)\n","            #update average validation loss \n","            valid_loss += loss.item()\n","            data.size(0)\n","            ##\n","        train_loss = train_loss/len(loaders['train'].dataset) ###\n","        train_loss_list.append(train_loss)\n","        train_acc_list.append(correct_train / total_train)\n","        valid_loss = valid_loss/len(loaders['valid'].dataset) ###\n","\n","\n","        # print training/validation statistics \n","        print('Epoch: {} \\tTraining Loss: {:.6f}\\tValidation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","        ## TODO: save the model if validation loss has decreased\n","        if valid_loss <= valid_loss_min:\n","            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            valid_loss_min,\n","            valid_loss))\n","            torch.save(model.state_dict(), save_path)\n","            valid_loss_min = valid_loss\n","\n","    # return trained model\n","    print(\"total training time: \", total_time)\n","    return model"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:05:28.661464Z","iopub.status.busy":"2023-03-21T21:05:28.661162Z","iopub.status.idle":"2023-03-21T21:13:16.824943Z","shell.execute_reply":"2023-03-21T21:13:16.822654Z","shell.execute_reply.started":"2023-03-21T21:05:28.661425Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1 \tTraining Loss: 0.039434\tValidation Loss: 0.037903\n","Validation loss decreased (inf --> 0.037903).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.035484\tValidation Loss: 0.035702\n","Validation loss decreased (0.037903 --> 0.035702).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.033455\tValidation Loss: 0.035418\n","Validation loss decreased (0.035702 --> 0.035418).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.033370\tValidation Loss: 0.036095\n","Epoch: 5 \tTraining Loss: 0.031487\tValidation Loss: 0.036072\n","Epoch: 6 \tTraining Loss: 0.030673\tValidation Loss: 0.036058\n","Epoch: 7 \tTraining Loss: 0.031685\tValidation Loss: 0.037441\n","Epoch: 8 \tTraining Loss: 0.030703\tValidation Loss: 0.037075\n","Epoch: 9 \tTraining Loss: 0.029536\tValidation Loss: 0.036708\n","Epoch: 10 \tTraining Loss: 0.029867\tValidation Loss: 0.037121\n","Epoch: 11 \tTraining Loss: 0.028830\tValidation Loss: 0.035839\n","Epoch: 12 \tTraining Loss: 0.028186\tValidation Loss: 0.036611\n","Epoch: 13 \tTraining Loss: 0.028716\tValidation Loss: 0.035902\n","Epoch: 14 \tTraining Loss: 0.028732\tValidation Loss: 0.037048\n","Epoch: 15 \tTraining Loss: 0.028654\tValidation Loss: 0.037634\n","Epoch: 16 \tTraining Loss: 0.027123\tValidation Loss: 0.035732\n","Epoch: 17 \tTraining Loss: 0.027349\tValidation Loss: 0.036416\n","Epoch: 18 \tTraining Loss: 0.027527\tValidation Loss: 0.040908\n","Epoch: 19 \tTraining Loss: 0.028690\tValidation Loss: 0.037222\n","Epoch: 20 \tTraining Loss: 0.027307\tValidation Loss: 0.037754\n","total training time:  373.1692186140008\n"]}],"source":["# train the model\n","import timeit\n","import numpy as np\n","model_transfer = train(20, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, '/kaggle/working/model_transfer.pt')\n"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:13:16.826835Z","iopub.status.busy":"2023-03-21T21:13:16.826364Z","iopub.status.idle":"2023-03-21T21:13:16.885031Z","shell.execute_reply":"2023-03-21T21:13:16.883656Z","shell.execute_reply.started":"2023-03-21T21:13:16.826796Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["model_transfer.load_state_dict(torch.load('/kaggle/working/model_transfer.pt'))"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:13:16.888308Z","iopub.status.busy":"2023-03-21T21:13:16.888029Z","iopub.status.idle":"2023-03-21T21:13:16.899980Z","shell.execute_reply":"2023-03-21T21:13:16.898901Z","shell.execute_reply.started":"2023-03-21T21:13:16.888282Z"},"trusted":true},"outputs":[],"source":["test_loss_list = []\n","test_acc_list = []\n","def test(loaders, model, criterion, use_cuda):\n","\n","    # monitor test loss and accuracy\n","    test_loss = 0.\n","   \n","    correct = 0.0\n","    total = 0.0\n","    t_time = 0\n","    model.eval()\n","    start = timeit.default_timer()\n","    for batch_idx, (data, target) in enumerate(loaders['test']):\n","        # move to GPU\n","        if use_cuda:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the loss\n","        loss = criterion(output, target)\n","        # update average test loss \n","        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n","        test_loss_list.append(test_loss)\n","        total += data.size(0)\n","        test_acc_list.append(correct / total)\n","        # convert output probabilities to predicted class\n","        pred = output.data.max(1, keepdim=True)[1]\n","        # compare predictions to true label\n","        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n","        print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n","        100. * correct / total, correct, total))\n","        \n","    stop = timeit.default_timer()\n","    print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n","        100. * correct / total, correct, total))\n","    print()\n","    print(\"total time \", stop-start)"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-03-21T21:15:51.429704Z","iopub.status.busy":"2023-03-21T21:15:51.429323Z","iopub.status.idle":"2023-03-21T21:16:03.702809Z","shell.execute_reply":"2023-03-21T21:16:03.701694Z","shell.execute_reply.started":"2023-03-21T21:15:51.429672Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test Accuracy: 53% (17/32)\n","\n","Test Accuracy: 57% (37/64)\n","\n","Test Accuracy: 59% (57/96)\n","\n","Test Accuracy: 57% (73/128)\n","\n","Test Accuracy: 57% (92/160)\n","\n","Test Accuracy: 55% (107/192)\n","\n","Test Accuracy: 53% (119/224)\n","\n","Test Accuracy: 54% (139/256)\n","\n","Test Accuracy: 52% (152/288)\n","\n","Test Accuracy: 51% (166/320)\n","\n","Test Accuracy: 50% (178/352)\n","\n","Test Accuracy: 50% (192/384)\n","\n","Test Accuracy: 50% (200/400)\n","Test Loss: 1.298897\n","\n","\n","Test Accuracy: 50% (200/400)\n","\n","total time  12.267449592000048\n"]}],"source":["test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
